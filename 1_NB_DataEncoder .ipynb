{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T09:16:42.839144Z",
     "start_time": "2023-08-12T09:16:42.834544Z"
    }
   },
   "outputs": [],
   "source": [
    "import scglue\n",
    "from scglue.models.nn import GraphConv # import Graph convolution\n",
    "from scglue.models.prob import ZILN, ZIN, ZINB\n",
    "from typing import Any, List, Mapping, NoReturn, Optional, Tuple\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:03:09.807221Z",
     "start_time": "2023-08-14T07:03:09.803748Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "from scglue.num import EPS\n",
    "# from scglue.models.sc import sc\n",
    "import scglue.models.glue as glue\n",
    "\n",
    "class DataDecoder(glue.DataDecoder):\n",
    "\n",
    "    r\"\"\"\n",
    "    Abstract data decoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_features\n",
    "        Output dimensionality\n",
    "    n_batches\n",
    "        Number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_features: int, n_batches: int = 1) -> None:  # pylint: disable=unused-argument\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(  # pylint: disable=arguments-differ\n",
    "            self, u: torch.Tensor, v: torch.Tensor,\n",
    "            b: torch.Tensor, l: Optional[torch.Tensor]\n",
    "    ) -> D.Normal:\n",
    "        r\"\"\"\n",
    "        Decode data from sample and feature latent\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        u\n",
    "            Sample latent\n",
    "        v\n",
    "            Feature latent\n",
    "        b\n",
    "            Batch index\n",
    "        l\n",
    "            Optional normalizer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        recon\n",
    "            Data reconstruction distribution\n",
    "        \"\"\"\n",
    "        raise NotImplementedError  # pragma: no cover\n",
    "        \n",
    "class NBDataDecoder(DataDecoder):\n",
    "\n",
    "    r\"\"\"\n",
    "    Negative binomial data decoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_features\n",
    "        Output dimensionality\n",
    "    n_batches\n",
    "        Number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_features: int, n_batches: int = 1) -> None:\n",
    "        super().__init__(out_features, n_batches)\n",
    "        self.scale_lin = torch.nn.Parameter(torch.zeros(n_batches, out_features))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(n_batches, out_features))\n",
    "        self.log_theta = torch.nn.Parameter(torch.zeros(n_batches, out_features))\n",
    "\n",
    "    def forward(\n",
    "            self, u: torch.Tensor, v: torch.Tensor,\n",
    "            b: torch.Tensor, l: torch.Tensor\n",
    "    ) -> D.NegativeBinomial:\n",
    "        #change [b] to [:,b]\n",
    "#         scale = F.softplus(self.scale_lin[b])\n",
    "        scale = F.softplus(self.scale_lin[:,b])\n",
    "        print(u.shape,v.t().shape,scale.shape)\n",
    "#         logit_mu = scale * (u @ v.t()) + self.bias[b]\n",
    "        logit_mu = scale * (u @ v.t()) + self.bias[:,b]\n",
    "        mu = F.softmax(logit_mu, dim=1) * l\n",
    "        log_theta = self.log_theta[:,b]\n",
    "        return D.NegativeBinomial(\n",
    "            log_theta.exp(),\n",
    "            logits=(mu + EPS).log() - log_theta\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:03:10.484365Z",
     "start_time": "2023-08-14T07:03:10.481432Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataEncoder(glue.DataEncoder):\n",
    "\n",
    "    r\"\"\"\n",
    "    Abstract data encoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features\n",
    "        Input dimensionality\n",
    "    out_features\n",
    "        Output dimensionality\n",
    "    h_depth\n",
    "        Hidden layer depth\n",
    "    h_dim\n",
    "        Hidden layer dimensionality\n",
    "    dropout\n",
    "        Dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, in_features: int, out_features: int,\n",
    "            h_depth: int = 2, h_dim: int = 256,\n",
    "            dropout: float = 0.2\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.h_depth = h_depth\n",
    "        ptr_dim = in_features\n",
    "        for layer in range(self.h_depth):\n",
    "            setattr(self, f\"linear_{layer}\", torch.nn.Linear(ptr_dim, h_dim))\n",
    "            setattr(self, f\"act_{layer}\", torch.nn.LeakyReLU(negative_slope=0.2))\n",
    "            setattr(self, f\"bn_{layer}\", torch.nn.BatchNorm1d(h_dim))\n",
    "            setattr(self, f\"dropout_{layer}\", torch.nn.Dropout(p=dropout))\n",
    "            ptr_dim = h_dim\n",
    "        self.loc = torch.nn.Linear(ptr_dim, out_features)\n",
    "        self.std_lin = torch.nn.Linear(ptr_dim, out_features)\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_l(self, x: torch.Tensor) -> Optional[torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Compute normalizer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            Input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        l\n",
    "            Normalizer\n",
    "        \"\"\"\n",
    "        raise NotImplementedError  # pragma: no cover\n",
    "\n",
    "    @abstractmethod\n",
    "    def normalize(\n",
    "            self, x: torch.Tensor, l: Optional[torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Normalize data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            Input data\n",
    "        l\n",
    "            Normalizer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        xnorm\n",
    "            Normalized data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError  # pragma: no cover\n",
    "\n",
    "    def forward(  # pylint: disable=arguments-differ\n",
    "            self, x: torch.Tensor, xrep: torch.Tensor,\n",
    "            lazy_normalizer: bool = True\n",
    "    ) -> Tuple[D.Normal, Optional[torch.Tensor]]:\n",
    "        r\"\"\"\n",
    "        Encode data to sample latent distribution\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            Input data\n",
    "        xrep\n",
    "            Alternative input data\n",
    "        lazy_normalizer\n",
    "            Whether to skip computing `x` normalizer (just return None)\n",
    "            if `xrep` is non-empty\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        u\n",
    "            Sample latent distribution\n",
    "        normalizer\n",
    "            Data normalizer\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        Normalization is always computed on `x`.\n",
    "        If xrep is empty, the normalized `x` will be used as input\n",
    "        to the encoder neural network, otherwise xrep is used instead.\n",
    "        \"\"\"\n",
    "        if xrep.numel():\n",
    "            l = None if lazy_normalizer else self.compute_l(x)\n",
    "            ptr = xrep\n",
    "        else:\n",
    "            l = self.compute_l(x)\n",
    "            ptr = self.normalize(x, l)\n",
    "        for layer in range(self.h_depth):\n",
    "            ptr = getattr(self, f\"linear_{layer}\")(ptr)\n",
    "            ptr = getattr(self, f\"act_{layer}\")(ptr)\n",
    "            ptr = getattr(self, f\"bn_{layer}\")(ptr)\n",
    "            ptr = getattr(self, f\"dropout_{layer}\")(ptr)\n",
    "        loc = self.loc(ptr)\n",
    "        std = F.softplus(self.std_lin(ptr)) + EPS\n",
    "        print('loc',loc)\n",
    "        print('std',std)\n",
    "        return D.Normal(loc, std), l\n",
    "#out[1] is l here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:03:10.817904Z",
     "start_time": "2023-08-14T07:03:10.814535Z"
    }
   },
   "outputs": [],
   "source": [
    "class NBDataEncoder(DataEncoder):\n",
    "\n",
    "    r\"\"\"\n",
    "    Data encoder for negative binomial data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features\n",
    "        Input dimensionality\n",
    "    out_features\n",
    "        Output dimensionality\n",
    "    h_depth\n",
    "        Hidden layer depth\n",
    "    h_dim\n",
    "        Hidden layer dimensionality\n",
    "    dropout\n",
    "        Dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    TOTAL_COUNT = 1e4\n",
    "\n",
    "    def compute_l(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.sum(dim=1, keepdim=True)\n",
    "\n",
    "    def normalize(\n",
    "            self, x: torch.Tensor, l: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return (x * (self.TOTAL_COUNT / l)).log1p()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1005,  1.0292,  0.1959, -0.6884],\n",
       "        [ 1.0902, -0.8609, -0.2302, -1.1643],\n",
       "        [ 0.8843, -0.3594, -0.5008, -0.3547],\n",
       "        [-0.6055,  1.2214, -0.3397, -0.0994],\n",
       "        [ 0.8620, -2.2344,  0.2287,  1.4481]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:20:13.127728Z",
     "start_time": "2023-08-14T07:20:13.122982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc tensor([[-0.2817,  0.3328],\n",
      "        [ 0.7076, -0.1204],\n",
      "        [-0.8935,  0.4211],\n",
      "        [ 1.2391, -0.2269],\n",
      "        [-0.7871, -0.5119],\n",
      "        [ 0.3025, -0.1633],\n",
      "        [ 0.1342, -0.2046],\n",
      "        [ 0.7545,  0.2591],\n",
      "        [-0.7137, -0.9024],\n",
      "        [ 0.1048, -0.1237]], grad_fn=<AddmmBackward0>)\n",
      "std tensor([[0.8224, 1.2542],\n",
      "        [0.2934, 0.6911],\n",
      "        [1.4690, 1.1622],\n",
      "        [0.4656, 0.4686],\n",
      "        [0.7069, 0.6685],\n",
      "        [1.1668, 0.8798],\n",
      "        [0.5931, 0.5736],\n",
      "        [0.6859, 0.5418],\n",
      "        [1.0748, 0.7364],\n",
      "        [0.6561, 0.8459]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[0.2, 0.5, 0.8], #1.5     for graph encoder each vertices have 3 features \n",
    "      [0.3, 0.7, 0.4], # 1.4\n",
    "      [0.6, 0.1, 0.9], # 1.6\n",
    "      [0.9, 0.2, 0.3]])# 1.4\n",
    "\n",
    "# if alternative x has non-zero elements\n",
    "#  xrep=torch.tensor([\n",
    "#        [0.6, 0.1, 0.9],\n",
    "#        [0.9, 0.2, 0.3]])\n",
    "xrep=torch.tensor([])\n",
    "xrep=torch.abs(torch.randn(10,4))\n",
    "encoder=NBDataEncoder(4,2)# Dataencoder: from 4 features to 2 features(in z)\n",
    "inputx=torch.abs(torch.randn(10,4)) # number of cell N=10,number of genes(vetices) |v1|=4,m=2\n",
    "\n",
    "u,l=encoder.forward(inputx,xrep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:47:17.727140Z",
     "start_time": "2023-08-14T07:47:17.720441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc tensor([[-0.4508, -0.8174],\n",
      "        [-0.2727, -0.7954],\n",
      "        [ 1.3175,  0.4944],\n",
      "        [-0.1101,  1.1146]], grad_fn=<AddmmBackward0>)\n",
      "std tensor([[1.0225, 1.2590],\n",
      "        [1.5359, 0.5046],\n",
      "        [0.3934, 1.0585],\n",
      "        [0.5192, 0.2525]], grad_fn=<AddBackward0>)\n",
      "u Normal(loc: torch.Size([4, 2]), scale: torch.Size([4, 2]))\n"
     ]
    }
   ],
   "source": [
    "# xrep=torch.tensor([])\n",
    "encoder=NBDataEncoder(3,2)# repeat above \n",
    "u,l=encoder.forward(x,xrep)\n",
    "print(\"u\",u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:03:11.556989Z",
     "start_time": "2023-08-14T07:03:11.552073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5000],\n",
      "        [1.4000],\n",
      "        [1.6000],\n",
      "        [1.4000]])\n",
      "tensor([[-0.4508, -0.8174],\n",
      "        [-0.2727, -0.7954],\n",
      "        [ 1.3175,  0.4944],\n",
      "        [-0.1101,  1.1146]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0225, 1.2590],\n",
      "        [1.5359, 0.5046],\n",
      "        [0.3934, 1.0585],\n",
      "        [0.5192, 0.2525]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(l)# sum of x \n",
    "print(u.loc)\n",
    "print(u.scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sc.py get sc.GraphEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:07:56.932564Z",
     "start_time": "2023-08-14T07:07:56.928638Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# class GraphEncoder0(torch.nn.Module):\n",
    "\n",
    "#     r\"\"\"\n",
    "#     Abstract graph encoder\n",
    "#     \"\"\"\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def forward(\n",
    "#             self, eidx: torch.Tensor, enorm: torch.Tensor, esgn: torch.Tensor\n",
    "#     ) -> D.Distribution:\n",
    "#         r\"\"\"\n",
    "#         Encode graph to vertex latent distribution\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         eidx\n",
    "#             Vertex indices of edges (:math:`2 \\times n_{edges}`)\n",
    "#         enorm\n",
    "#             Normalized weight of edges (:math:`n_{edges}`)\n",
    "#         esgn\n",
    "#             Sign of edges (:math:`n_{edges}`)\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         v\n",
    "#             Vertex latent distribution\n",
    "#             (:math:`n_{vertices} \\times n_{features}`)\n",
    "#         \"\"\"\n",
    "#         raise NotImplementedError  # pragma: no cover\n",
    "\n",
    "\n",
    "\n",
    "class GraphEncoder(glue.GraphEncoder):\n",
    "\n",
    "    r\"\"\"\n",
    "    Graph encoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vnum\n",
    "        Number of vertices\n",
    "    out_features\n",
    "        Output dimensionality\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, vnum: int, out_features: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.vrepr = torch.nn.Parameter(torch.zeros(vnum, out_features))\n",
    "        self.conv = GraphConv()#图卷积\n",
    "        self.loc = torch.nn.Linear(out_features, out_features) #two fully connected layer\n",
    "        self.std_lin = torch.nn.Linear(out_features, out_features)\n",
    "\n",
    "    def forward(# output is v\n",
    "            self, eidx: torch.Tensor, enorm: torch.Tensor, esgn: torch.Tensor\n",
    "    ) -> D.Normal:\n",
    "        ptr = self.conv(self.vrepr, eidx, enorm, esgn) #output of graph convolution\n",
    "        loc = self.loc(ptr) # for mean and variance using two linear layer\n",
    "        std = F.softplus(self.std_lin(ptr)) + 1e-7\n",
    "        print('Normal sample: ',D.Normal(loc, std).sample_n(2,))\n",
    "        print('Normal sample: ',D.Normal(loc, std).sample())\n",
    "        print('//////////////')\n",
    "        return D.Normal(loc, std) #output a normal distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:21:10.378290Z",
     "start_time": "2023-08-14T07:21:10.372126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal sample:  tensor([[[ 7.2776e-01,  8.3075e-01],\n",
      "         [ 6.5701e-02,  6.9408e-01],\n",
      "         [-5.2981e-01, -1.2566e+00],\n",
      "         [-3.0500e-01,  4.1168e-04]],\n",
      "\n",
      "        [[ 5.3128e-01,  2.6281e-01],\n",
      "         [-4.7231e-01, -4.8832e-01],\n",
      "         [ 5.5304e-01, -2.2238e-01],\n",
      "         [ 2.3909e-01, -1.6918e-01]]])\n",
      "Normal sample:  tensor([[-0.5816, -0.2778],\n",
      "        [-0.3754,  0.0418],\n",
      "        [-0.5096, -0.8488],\n",
      "        [-0.0242, -0.9460]])\n",
      "//////////////\n",
      "tensor([[-0.0960, -0.0845],\n",
      "        [-0.0960, -0.0845],\n",
      "        [-0.0960, -0.0845],\n",
      "        [-0.0960, -0.0845]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4889, 0.5951],\n",
      "        [0.4889, 0.5951],\n",
      "        [0.4889, 0.5951],\n",
      "        [0.4889, 0.5951]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.1959, -0.0101],\n",
      "        [-0.6630,  0.1759],\n",
      "        [-0.0585, -0.3054],\n",
      "        [ 0.6951,  1.0880]])\n"
     ]
    }
   ],
   "source": [
    "#to get graph embedding v\n",
    "#the graph has 4 vertices and 5 edges\n",
    "edges =torch.tensor([\n",
    "    [0, 1],  # 边1连接节点0和节点1\n",
    "    [1, 2],  # 边2连接节点1和节点2\n",
    "    [2, 0],  # 边3连接节点2和节点0\n",
    "    [1, 3],  # 边4连接节点1和节点3\n",
    "    [3, 0],  # 边5连接节点3和节点0\n",
    "])\n",
    "# the number of genes is size of vertices and m=2\n",
    "gencoder=GraphEncoder(4,2)# 4 vertices and out_features is 2\n",
    "#weight on edges \n",
    "weight=torch.tensor([[0.6,0.3,0.7,0.5,0.9]]) # toy weight\n",
    "weight=torch.flatten(F.normalize(weight))\n",
    "#signs on edges \n",
    "sign=torch.flatten(torch.tensor([[1,1,1,-1,1]]))\n",
    "# model=GraphConv()\n",
    "# model.forward(x,torch.transpose(edges,0,1),weight,sign)\n",
    "# to use graph encoder to get embedding/distribution v (|v1|=4,m=2)\n",
    "v=gencoder.forward(torch.transpose(edges,0,1),weight,sign)\n",
    "print(v.loc)\n",
    "print(v.scale)\n",
    "print(v.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:45:32.345391Z",
     "start_time": "2023-08-14T07:45:32.339567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8182, -0.3265],\n",
      "        [ 0.2091,  0.3257],\n",
      "        [-1.5657, -0.2676],\n",
      "        [-0.8431, -1.1648]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.6640681 , -0.30737612],\n",
       "       [-0.6640681 , -0.30737612],\n",
       "       [-0.6640681 , -0.30737612],\n",
       "       [-0.6640681 , -0.30737612]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample=2 # some tests \n",
    "zv=torch.cat([v.sample((1, )).cpu() for _ in range(n_sample)\n",
    "            ]).permute(1, 0, 2).numpy()\n",
    "zv=v.mean.detach().cpu().numpy()\n",
    "print(v.sample())\n",
    "zv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NBDataDecoder(DataDecoder):\n",
    "\n",
    "    r\"\"\"\n",
    "    Negative binomial data decoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    out_features\n",
    "        Output dimensionality\n",
    "    n_batches\n",
    "        Number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_features: int, n_batches: int = 1) -> None:\n",
    "        super().__init__(out_features, n_batches=n_batches)\n",
    "        self.scale_lin = torch.nn.Parameter(torch.zeros(n_batches, out_features))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(n_batches, out_features))\n",
    "        self.log_theta = torch.nn.Parameter(torch.zeros(n_batches, out_features))\n",
    "\n",
    "    def forward(\n",
    "            self, u: torch.Tensor, v: torch.Tensor,\n",
    "            b: torch.Tensor, l: torch.Tensor\n",
    "    ) -> D.NegativeBinomial:\n",
    "        scale = F.softplus(self.scale_lin[b])\n",
    "        print(u.shape,v.t().shape,scale.shape)\n",
    "        print(scale, self.scale_lin)\n",
    "        #torch.Size([128, 50]) torch.Size([50, 800]) torch.Size([128, 800])\n",
    "        logit_mu = scale * (u @ v.t()) + self.bias[b]\n",
    "        mu = F.softmax(logit_mu, dim=1) * l\n",
    "        log_theta = self.log_theta[b]\n",
    "        return D.NegativeBinomial(\n",
    "            log_theta.exp(),\n",
    "            logits=(mu + EPS).log() - log_theta\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:29:09.590893Z",
     "start_time": "2023-08-14T07:29:09.568513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2]) torch.Size([2, 4]) torch.Size([10, 4])\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931],\n",
      "        [0.6931, 0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>) Parameter containing:\n",
      "tensor([[0., 0., 0., 0.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3961, 0.3183, 0.4803, 0.5708],\n",
       "        [0.2632, 0.0734, 0.6305, 1.5664],\n",
       "        [0.4310, 1.1609, 0.2773, 0.1500],\n",
       "        [1.4730, 1.1394, 0.5624, 0.4345],\n",
       "        [0.4045, 0.8802, 0.2500, 0.1465],\n",
       "        [1.7561, 0.5880, 0.9617, 1.2429],\n",
       "        [0.2681, 1.0977, 0.2508, 0.1300],\n",
       "        [0.5110, 3.1383, 0.9444, 0.5316],\n",
       "        [1.3598, 2.4380, 0.8561, 0.5510],\n",
       "        [1.3997, 0.6251, 1.2062, 1.6334]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decoder need graph embedding v\n",
    "from scglue.models import data\n",
    "decoder=NBDataDecoder(4)# out_feature is 4\n",
    "b=torch.tensor([0,0,0,0,0,0,0,0,0,0])# of size 10\n",
    "    \n",
    "decoder.forward(u.sample(),v.sample(),b,l).mean #size 10,4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:21:12.608686Z",
     "start_time": "2023-08-14T07:21:12.601438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1177, 0.2614, 0.3580, 0.0284],\n",
       "        [0.1492, 1.2228, 0.1198, 1.0416],\n",
       "        [0.4966, 0.5017, 0.6235, 0.3974],\n",
       "        [1.4366, 1.0217, 0.5163, 0.6349],\n",
       "        [0.6009, 0.2623, 0.6252, 0.1928],\n",
       "        [1.3275, 1.4076, 1.3831, 0.4304],\n",
       "        [0.2682, 0.2548, 0.8072, 0.4164],\n",
       "        [0.9626, 1.1145, 2.9090, 0.1393],\n",
       "        [0.8406, 1.9030, 1.7614, 0.6999],\n",
       "        [1.7558, 1.8256, 0.1858, 1.0971]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T07:12:42.286297Z",
     "start_time": "2023-08-14T07:12:42.281659Z"
    }
   },
   "outputs": [],
   "source": [
    "v.sample().t().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl=torch.nn.Parameter(torch.rand(1, 7))\n",
    "scl.shape\n",
    "b = torch.tensor([1, 3, 4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSCGLUEModel\u001b[39;00m(\u001b[43mModel\u001b[49m):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    GLUE model for single-cell multi-omics data integration\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m        Random seed\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     NET_TYPE \u001b[38;5;241m=\u001b[39m SCGLUE\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "class SCGLUEModel(Model):\n",
    "\n",
    "    r\"\"\"\n",
    "    GLUE model for single-cell multi-omics data integration\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adatas\n",
    "        Datasets (indexed by modality name)\n",
    "    vertices\n",
    "        Guidance graph vertices (must cover feature names in all modalities)\n",
    "    latent_dim\n",
    "        Latent dimensionality\n",
    "    h_depth\n",
    "        Hidden layer depth for encoder and discriminator\n",
    "    h_dim\n",
    "        Hidden layer dimensionality for encoder and discriminator\n",
    "    dropout\n",
    "        Dropout rate\n",
    "    shared_batches\n",
    "        Whether the same batches are shared across modalities\n",
    "    random_seed\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "\n",
    "    NET_TYPE = SCGLUE\n",
    "    TRAINER_TYPE = SCGLUETrainer\n",
    "\n",
    "    GRAPH_BATCHES: int = 32  # Number of graph batches in each graph epoch\n",
    "    ALIGN_BURNIN_PRG: float = 8.0  # Effective optimization progress of align_burnin (learning rate * iterations)\n",
    "    MAX_EPOCHS_PRG: float = 48.0  # Effective optimization progress of max_epochs (learning rate * iterations)\n",
    "    PATIENCE_PRG: float = 4.0  # Effective optimization progress of patience (learning rate * iterations)\n",
    "    REDUCE_LR_PATIENCE_PRG: float = 2.0  # Effective optimization progress of reduce_lr_patience (learning rate * iterations)\n",
    "\n",
    "    def __init__(\n",
    "            self, adatas: Mapping[str, AnnData],\n",
    "            vertices: List[str], latent_dim: int = 50,\n",
    "            h_depth: int = 2, h_dim: int = 256,\n",
    "            dropout: float = 0.2, shared_batches: bool = False,\n",
    "            random_seed: int = 0\n",
    "    ) -> None:\n",
    "        # print(vertices)\n",
    "        self.vertices = pd.Index(vertices)\n",
    "        # print('vertices: ',self.vertices)\n",
    "        self.random_seed = random_seed\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "        g2v = sc.GraphEncoder(self.vertices.size, latent_dim)# latent_dim:50\n",
    "        v2g = sc.GraphDecoder()# modalities: rna: batches, features\n",
    "        self.modalities, idx, x2u, u2x, all_ct = {}, {}, {}, {}, set()\n",
    "        for k, adata in adatas.items(): #k is keys 'rna' or 'atac' and adata is value\n",
    "            if config.ANNDATA_KEY not in adata.uns:\n",
    "                raise ValueError(\n",
    "                    f\"The '{k}' dataset has not been configured. \"\n",
    "                    f\"Please call `configure_dataset` first!\"\n",
    "                )\n",
    "            data_config = copy.deepcopy(adata.uns[config.ANNDATA_KEY])# by config.ANNDATA_KEY\n",
    "            print('data_config', data_config)    #'prob_model': 'NB', 'use_highly_variable': True, 'features':\n",
    "            if data_config[\"rep_dim\"] and data_config[\"rep_dim\"] < latent_dim:\n",
    "                self.logger.warning(\n",
    "                    \"It is recommended that `use_rep` dimensionality \"\n",
    "                    \"be equal or larger than `latent_dim`.\"\n",
    "                ) # store vertex feature index into idx classified by modalities\n",
    "            idx[k] = self.vertices.get_indexer(data_config[\"features\"]).astype(np.int64)#rna/atac's gene index\n",
    "            if idx[k].min() < 0:\n",
    "                raise ValueError(\"Not all modality features exist in the graph!\")\n",
    "            idx[k] = torch.as_tensor(idx[k])#_Encoder_map all from sc.py\n",
    "            x2u[k] = _ENCODER_MAP[data_config[\"prob_model\"]](\n",
    "                data_config[\"rep_dim\"] or len(data_config[\"features\"]), latent_dim,\n",
    "                h_depth=h_depth, h_dim=h_dim, dropout=dropout\n",
    "            )\n",
    "            data_config[\"batches\"] = pd.Index([]) if data_config[\"batches\"] is None \\\n",
    "                else pd.Index(data_config[\"batches\"]) #data_config contain 'batches', 'rep_dim','features'\n",
    "            u2x[k] = _DECODER_MAP[data_config[\"prob_model\"]](#data decoder\n",
    "                len(data_config[\"features\"]),\n",
    "                n_batches=max(data_config[\"batches\"].size, 1)\n",
    "            )\n",
    "            all_ct = all_ct.union(\n",
    "                set() if data_config[\"cell_types\"] is None\n",
    "                else data_config[\"cell_types\"]\n",
    "            )\n",
    "            self.modalities[k] = data_config\n",
    "            # print('modalities', self.modalities)#{'rna': {'prob_model': 'NB', 'use_highly_variable': True, 'features':\n",
    "\n",
    "        all_ct = pd.Index(all_ct).sort_values()\n",
    "        for modality in self.modalities.values():\n",
    "            modality[\"cell_types\"] = all_ct\n",
    "        if shared_batches:\n",
    "            all_batches = [modality[\"batches\"] for modality in self.modalities.values()]\n",
    "            ref_batch = all_batches[0]\n",
    "            for batches in all_batches:\n",
    "                if not np.array_equal(batches, ref_batch):\n",
    "                    raise RuntimeError(\"Batches must match when using `shared_batches`!\")\n",
    "            du_n_batches = ref_batch.size\n",
    "        else:\n",
    "            du_n_batches = 0\n",
    "        du = sc.Discriminator(\n",
    "            latent_dim, len(self.modalities), n_batches=du_n_batches,\n",
    "            h_depth=h_depth, h_dim=h_dim, dropout=dropout\n",
    "        )\n",
    "        prior = sc.Prior()\n",
    "        super().__init__( #base object stored as derived class\n",
    "            g2v, v2g, x2u, u2x, idx, du, prior,\n",
    "            u2c=None if all_ct.empty else sc.Classifier(latent_dim, all_ct.size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight\n",
      "linear.bias\n",
      "buffer1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import chain\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "        self.register_buffer('buffer1', torch.tensor([1.0, 2.0, 3.0]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# 遍历目标模型的参数和命名缓冲区，并打印名称和张量\n",
    "for name, tensor in chain(model.named_parameters(), model.named_buffers()):\n",
    "    print(name)\n",
    "#     print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2956,  0.2632, -0.0399, -0.0600, -0.2162,  0.1683,  0.1146,  0.2986,\n",
      "         -0.0525, -0.0917],\n",
      "        [ 0.2347,  0.0077,  0.2026, -0.1359,  0.2513,  0.2200,  0.2738,  0.3159,\n",
      "         -0.2503,  0.0249],\n",
      "        [-0.1529,  0.1079, -0.0804, -0.1616,  0.1510,  0.0086,  0.0306, -0.0593,\n",
      "         -0.1751,  0.0961],\n",
      "        [ 0.0552,  0.1704,  0.3098,  0.0919, -0.2823, -0.1996,  0.2215,  0.2133,\n",
      "         -0.2431,  0.1442],\n",
      "        [-0.0587, -0.0956, -0.0817,  0.2263, -0.3023,  0.0004,  0.0160, -0.2632,\n",
      "         -0.1567,  0.1490]], requires_grad=True)\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n",
      "Parameter containing:\n",
      "tensor([ 0.2092, -0.1887,  0.2704,  0.0862,  0.2528], requires_grad=True)\n",
      "tensor([True, True, True, True, True])\n",
      "tensor([1., 2., 3.])\n",
      "tensor([True, True, True])\n"
     ]
    }
   ],
   "source": [
    "for name, tensor in chain(model.named_parameters(), model.named_buffers()):\n",
    "    print(tensor)\n",
    "    if isinstance(name, torch.nn.Parameter):# if t is an instance of nn.Parameter\n",
    "        t = name.data \n",
    "        print(\"t\",t)\n",
    "    print(tensor==tensor.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27246c2c78c04065bf3cea0f54c5ecdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1\n",
      "Training Epoch: 2\n",
      "Training Epoch: 3\n",
      "Training Epoch: 4\n",
      "Training Epoch: 5\n",
      "Training Epoch: 6\n",
      "Training Epoch: 7\n",
      "Training Epoch: 8\n",
      "Training Epoch: 9\n",
      "Training Epoch: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 7820\n",
       "\tepoch: 10\n",
       "\tepoch_length: 782\n",
       "\tmax_epochs: 10\n",
       "\toutput: 2\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "\n",
    "def update_fn(engine, batch):\n",
    "    # 在此处执行训练逻辑\n",
    "#     print('training process')\n",
    "    return 2\n",
    "\n",
    "# 创建训练引擎\n",
    "trainer = Engine(update_fn)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_epoch(engine):\n",
    "    epoch = engine.state.epoch\n",
    "    print(f\"Training Epoch: {epoch}\")\n",
    "\n",
    "# 执行训练过程\n",
    "trainer.run(train_dataloader, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa_base2",
   "language": "python",
   "name": "fa_base2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
